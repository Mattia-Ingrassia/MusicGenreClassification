{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ceedf0b",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ea7e6d",
   "metadata": {},
   "source": [
    "First, we import some useful libraries and set a random seed to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f02e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "import json\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.util import random_noise\n",
    "\n",
    "\n",
    "\n",
    "# Set random state for reproducibility\n",
    "random_state = 23\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589a10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF YOU ARE NOT USING KAGGLE\n",
    "BASE_PATH = kagglehub.dataset_download(\"andradaolteanu/gtzan-dataset-music-genre-classification\")\n",
    "BASE_PATH += \"/Data/\"\n",
    "OUT_BASE_PATH = \"./\"\n",
    "# IF YOU ARE USING KAGGLE, ADD THE ORIGINAL DATASET TO THE NOTEBOOOK, COMMENT THE TWO PREVIOUS LINES AND UNCOMMENT THE FOLLOWING ONE\n",
    "# BASE_PATH = \"/kaggle/input/gtzan-dataset-music-genre-classification/Data\"\n",
    "# OUT_BASE_PATH = \"/kaggle/working/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fd146b",
   "metadata": {},
   "source": [
    "Next, we prepare the environment by defining the variables needed and loading the dataset from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36f4e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.0).\n",
      "[0 1 2 3 4 5 6 7 8 9] [100 100 100 100 100  99 100 100 100 100]\n"
     ]
    }
   ],
   "source": [
    "# Set up save paths\n",
    "original_path = \"original/\"\n",
    "original_aug = \"original_aug/\"\n",
    "cropped_path = \"cropped/\"\n",
    "cropped_aug = \"cropped_aug/\"\n",
    "cropped_aug_inj = \"cropped_aug_inj/\"\n",
    "\n",
    "original_img_shape = (288, 432, 4)\n",
    "\n",
    "CONFIGS = [original_path, original_aug, cropped_path, cropped_aug, cropped_aug_inj]\n",
    "OUT_DIR = OUT_BASE_PATH + \"Augmented_GTZAN/\"\n",
    "SPLITS = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "genres = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "num_classes = len(genres)\n",
    "n_images_per_genre = 100\n",
    "genre_indices = list(range(num_classes))\n",
    "\n",
    "# One instance is removed (jazz-54) since it was corrupted\n",
    "X = np.zeros((n_images_per_genre*num_classes - 1, *original_img_shape))\n",
    "y = np.array([genre_idx \n",
    "              for genre_idx in genre_indices  \n",
    "            for _ in range(n_images_per_genre)])\n",
    "# We are deleting the corrupted instance \n",
    "y = np.delete(y, 500)\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(unique, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "569f64a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_imgs(X, y, genres, out_dir, split, indices=None, num_classes=10):\n",
    "    y_cat = to_categorical(y)\n",
    "    labels = np.argmax(y_cat, axis=1)\n",
    "    genre_counters = [0 for _ in range(num_classes)]\n",
    "    new_indices = np.empty((len(X), 2), dtype=object)\n",
    "\n",
    "    for instance_idx, (img, label) in enumerate(zip(X, labels)):\n",
    "\n",
    "        genre = genres[label]\n",
    "        idx = genre_counters[label]\n",
    "        filename = f\"{genre}{idx:05d}.png\"\n",
    "\n",
    "        save_path = os.path.join(out_dir, split, genre, filename)\n",
    "\n",
    "        if indices is not None:\n",
    "            new_indices[instance_idx][0] = indices[instance_idx]\n",
    "            save_path = save_path.replace(\"\\\\\", \"/\")\n",
    "            new_indices[instance_idx][1] = save_path\n",
    "\n",
    "        mpimg.imsave(save_path, img)\n",
    "        genre_counters[label] += 1\n",
    "\n",
    "    if indices is not None:\n",
    "        return new_indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471802ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_index = 0\n",
    "\n",
    "for genre_name in genres:\n",
    "    for image_number in range(n_images_per_genre):\n",
    "        # Skip corrupted file\n",
    "        if genre_name == 'jazz' and image_number == 54:\n",
    "            continue\n",
    "        curr_path = f\"{BASE_PATH}/images_original/{genre_name}/{genre_name}{image_number:05d}.png\"\n",
    "        img = mpimg.imread(curr_path)\n",
    "        X[img_index, :, :, :] = img\n",
    "        img_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a6bea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the out folders\n",
    "for conf in CONFIGS:\n",
    "    for split in SPLITS:\n",
    "        for genre_name in genres:\n",
    "            os.makedirs(os.path.join(OUT_DIR, conf, split, genre_name), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "360568c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tiain\\Desktop\\ppppppp\\.venv\\lib\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: FrequencyMasking is a specialized version of XYMasking. For more flexibility (multiple masks, custom fill values, time masking), consider using XYMasking directly from albumentations.XYMasking.\n",
      "  original_init(self, **validated_kwargs)\n",
      "c:\\Users\\tiain\\Desktop\\ppppppp\\.venv\\lib\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: TimeMasking is a specialized version of XYMasking. For more flexibility (multiple masks, custom fill values, frequency masking), consider using XYMasking directly from albumentations.XYMasking.\n",
      "  original_init(self, **validated_kwargs)\n",
      "c:\\Users\\tiain\\Desktop\\ppppppp\\.venv\\lib\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: TimeReverse is an alias for HorizontalFlip transform. Consider using HorizontalFlip directly from albumentations.HorizontalFlip. \n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    }
   ],
   "source": [
    "pipeline_transform = A.Compose([\n",
    "    A.FrequencyMasking(freq_mask_param = 50, p=0.6),\n",
    "    A.TimeMasking(time_mask_param = 65, p=0.6),\n",
    "    A.TimeReverse(p=0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca79dd35",
   "metadata": {},
   "source": [
    "## Original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a69cd7c",
   "metadata": {},
   "source": [
    "First, we divide the original dataset by saving indexes in the relative splits wihtout applying any modification to the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ef3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(719, 288, 432, 4) (719,) (200, 288, 432, 4) (200,) (80, 288, 432, 4) (80,)\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.20\n",
    "val_size = 0.1\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X, y, range(len(X)), test_size = test_size, random_state = random_state, stratify=y)\n",
    "X_train, X_val, y_train, y_val, indices_train, indices_val = train_test_split(X_train, y_train, indices_train, test_size = val_size, random_state = random_state, stratify=y_train)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, X_val.shape, y_val.shape)\n",
    "\n",
    "# Train\n",
    "train_ind = save_imgs(X_train, y_train, genres, OUT_DIR+original_path, SPLITS[0], indices=indices_train)\n",
    "# Validation\n",
    "val_ind = save_imgs(X_val, y_val, genres, OUT_DIR+original_path, SPLITS[1], indices=indices_val)\n",
    "# Test\n",
    "test_ind = save_imgs(X_test, y_test, genres, OUT_DIR+original_path, SPLITS[2], indices=indices_test)\n",
    "\n",
    "\n",
    "with open(OUT_BASE_PATH + \"indices_train.json\", 'w+') as f:\n",
    "    f.write(json.dumps(train_ind))\n",
    "with open(OUT_BASE_PATH + \"indices_test.json\",'w+') as f:\n",
    "    f.write(json.dumps(test_ind))\n",
    "with open(OUT_BASE_PATH + \"indices_val.json\", 'w+') as f:\n",
    "    f.write(json.dumps(val_ind))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d527a418",
   "metadata": {},
   "source": [
    "## 1 - Original Dataset Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d6f17ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(559, 288, 432, 4) (559,) (300, 288, 432, 4) (300,) (140, 288, 432, 4) (140,)\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.30\n",
    "val_size = 0.20\n",
    "X_train_res, X_test_res, y_train_res, y_test_res = train_test_split(X, y, test_size = test_size, random_state = random_state, stratify=y)\n",
    "X_train_res, X_val_res, y_train_res, y_val_res = train_test_split(X_train_res, y_train_res, test_size = val_size, random_state = random_state, stratify=y_train_res)\n",
    "print(X_train_res.shape, y_train_res.shape, X_test_res.shape, y_test_res.shape, X_val_res.shape, y_val_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0b27980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Num imgs is the number of images to reach after augmentation\n",
    "num_imgs = len(X_train_res) * 3\n",
    "X_train_transformed = np.zeros((num_imgs, *original_img_shape))\n",
    "y_train_transformed = np.zeros((num_imgs))\n",
    "\n",
    "# First, we copy the original imgs\n",
    "X_train_transformed[:len(X_train_res), :, :, :] = X_train_res[:, :, :, :]\n",
    "y_train_transformed[:len(y_train_res)] = y_train_res[:]\n",
    "\n",
    "# Then, we add augmented images\n",
    "cur_idx = len(X_train_res)\n",
    "while cur_idx < num_imgs:\n",
    "    for image, label in zip(X_train_res, y_train_res):\n",
    "        transformed_image = pipeline_transform(image=image)[\"image\"]\n",
    "        X_train_transformed[cur_idx, :, :, :] = transformed_image[:, :, :]\n",
    "        y_train_transformed[cur_idx] = label\n",
    "        cur_idx = cur_idx + 1\n",
    "        # Exit when the number of images is reached\n",
    "        if cur_idx == num_imgs:\n",
    "            break\n",
    "    \n",
    "# Train\n",
    "save_imgs(X_train_transformed, y_train_transformed, genres, OUT_DIR+original_aug, SPLITS[0])\n",
    "# Validation\n",
    "save_imgs(X_val_res, y_val_res, genres, OUT_DIR+original_aug, SPLITS[1])\n",
    "# Test\n",
    "save_imgs(X_test_res, y_test_res, genres, OUT_DIR+original_aug, SPLITS[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c3914f",
   "metadata": {},
   "source": [
    "## 2 - Cropped Images Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc0a77f",
   "metadata": {},
   "source": [
    "Since the images include white space around the spectrograms, we removed these regions to check whether excluding irrelevant information improves the neural network performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "76746b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "54\n",
      "35\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "img = X[0]\n",
    "top_padding = 0\n",
    "bottom_padding = 0\n",
    "left_padding = 0\n",
    "right_padding = 0\n",
    "GRAY_THRESHOLD = 0.99\n",
    "IMG_H = img.shape[0]\n",
    "IMG_W = img.shape[1]\n",
    "CENTER_H = IMG_H // 2\n",
    "CENTER_W = IMG_W // 2\n",
    "\n",
    "\n",
    "# Y' = 0.299 R + 0.587 G + 0.114 B \n",
    "gray_img = 0.299 * img[:, :, 0] + 0.587 * img[:, :, 1] +  0.114 * img[:, :, 2]\n",
    "\n",
    "\n",
    "\n",
    "# Top padding\n",
    "for i in range(IMG_H):\n",
    "    if gray_img[i, CENTER_W] < GRAY_THRESHOLD:\n",
    "        top_padding = i\n",
    "        break\n",
    "\n",
    "# Left padding\n",
    "for i in range(IMG_W):\n",
    "    if gray_img[CENTER_H, i] < GRAY_THRESHOLD:\n",
    "        left_padding = i\n",
    "        break\n",
    "\n",
    "# Bottom padding\n",
    "for i in range(IMG_H):\n",
    "    if gray_img[IMG_H - 1 - i, CENTER_W] < GRAY_THRESHOLD:\n",
    "        bottom_padding = i\n",
    "        break\n",
    "\n",
    "# Right padding\n",
    "for i in range(IMG_W):\n",
    "    if gray_img[CENTER_H, IMG_W - 1 - i] < GRAY_THRESHOLD:\n",
    "        right_padding = i\n",
    "        break\n",
    "\n",
    "print(top_padding) #35\n",
    "print(left_padding) #54\n",
    "print(bottom_padding) #35\n",
    "print(right_padding) #42\n",
    "\n",
    "# Need an extra pixel for bottom and right\n",
    "bottom_padding += 1\n",
    "right_padding += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3064d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(719, 218, 335, 4) (719,) (200, 218, 335, 4) (200,) (80, 218, 335, 4) (80,)\n"
     ]
    }
   ],
   "source": [
    "cropped_img_shape = (original_img_shape[0] - top_padding - top_padding, original_img_shape[1] - left_padding - right_padding, original_img_shape[2])\n",
    "\n",
    "X_resized = np.zeros((n_images_per_genre*num_classes - 1, *cropped_img_shape))\n",
    "\n",
    "i = 0\n",
    "for img in X:\n",
    "    X_resized[i, :, :, :] = img[top_padding:-top_padding, left_padding:-right_padding, :]\n",
    "    i = i + 1\n",
    "\n",
    "test_size = 0.20\n",
    "val_size = 0.1\n",
    "X_train_res, X_test_res, y_train_res, y_test_res = train_test_split(X_resized, y, test_size = test_size, random_state = random_state, stratify=y)\n",
    "X_train_res, X_val_res, y_train_res, y_val_res = train_test_split(X_train_res, y_train_res, test_size = val_size, random_state = random_state, stratify=y_train_res)\n",
    "print(X_train_res.shape, y_train_res.shape, X_test_res.shape, y_test_res.shape, X_val_res.shape, y_val_res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd95edc0",
   "metadata": {},
   "source": [
    "Here we save the cropped images (not augmented yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6e3b4ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2. RESIZED\n",
    "# Train\n",
    "save_imgs(X_train_res, y_train_res, genres, OUT_DIR+cropped_path, SPLITS[0])\n",
    "# Validation\n",
    "save_imgs(X_val_res, y_val_res, genres, OUT_DIR+cropped_path, SPLITS[1])\n",
    "# Test\n",
    "save_imgs(X_test_res, y_test_res, genres, OUT_DIR+cropped_path, SPLITS[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b4a11e",
   "metadata": {},
   "source": [
    "And now we can augment the cropped images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd862460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(559, 218, 335, 4) (559,) (300, 218, 335, 4) (300,) (140, 218, 335, 4) (140,)\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.30\n",
    "val_size = 0.20\n",
    "X_train_res, X_test_res, y_train_res, y_test_res = train_test_split(X_resized, y, test_size = test_size, random_state = random_state, stratify=y)\n",
    "X_train_res, X_val_res, y_train_res, y_val_res = train_test_split(X_train_res, y_train_res, test_size = val_size, random_state = random_state, stratify=y_train_res)\n",
    "print(X_train_res.shape, y_train_res.shape, X_test_res.shape, y_test_res.shape, X_val_res.shape, y_val_res.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "24d55710",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imgs = len(X_train_res) * 3\n",
    "X_train_transformed = np.zeros((num_imgs, *cropped_img_shape))\n",
    "y_train_transformed = np.zeros((num_imgs))\n",
    "\n",
    "# Copy the original imgs\n",
    "X_train_transformed[:len(X_train_res), :, :, :] = X_train_res[:, :, :, :]\n",
    "y_train_transformed[:len(y_train_res)] = y_train_res[:]\n",
    "\n",
    "i = len(X_train_res)\n",
    "while i < num_imgs:\n",
    "    for image, label in zip(X_train_res, y_train_res):\n",
    "        transformed_image = pipeline_transform(image=image)[\"image\"]\n",
    "        X_train_transformed[i, :, :, :] = transformed_image[:, :, :]\n",
    "        y_train_transformed[i] = label\n",
    "        i = i + 1\n",
    "        if i == num_imgs:\n",
    "            break\n",
    "\n",
    "#### 3. AUGMENTATION\n",
    "# Train\n",
    "save_imgs(X_train_transformed, y_train_transformed, genres, OUT_DIR+cropped_aug, SPLITS[0])\n",
    "# Validation\n",
    "save_imgs(X_val_res, y_val_res, genres, OUT_DIR+cropped_aug, SPLITS[1])\n",
    "# Test\n",
    "save_imgs(X_test_res, y_test_res, genres, OUT_DIR+cropped_aug, SPLITS[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa3d583",
   "metadata": {},
   "source": [
    "## 3 - Cropped Data Augmentation + Noise Injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3a7d3863",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imgs = len(X_train_transformed) * 2\n",
    "X_train_transformed_noise = np.zeros((num_imgs, *cropped_img_shape))\n",
    "y_train_transformed_noise = np.zeros((num_imgs))\n",
    "\n",
    "# Copy the original imgs\n",
    "X_train_transformed_noise[:len(X_train_transformed), :, :, :] = X_train_transformed[:, :, :, :]\n",
    "y_train_transformed_noise[:len(y_train_transformed)] = y_train_transformed[:]\n",
    "\n",
    "noise_modes = [\"s&p\", \"gaussian\", \"poisson\"]\n",
    "\n",
    "\n",
    "i = len(X_train_transformed)\n",
    "while i < num_imgs:\n",
    "    for image, label in zip(X_train_transformed, y_train_transformed):\n",
    "\n",
    "        n = random.randint(0, 2)\n",
    "        \n",
    "        # Add choosen noise to the image\n",
    "        noise = random_noise(image, mode=noise_modes[n])\n",
    "        \n",
    "        X_train_transformed_noise[i, :, :, :] = noise[:, :, :]\n",
    "        y_train_transformed_noise[i] = label\n",
    "        i = i + 1\n",
    "        if i == num_imgs:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2a3d0a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 4. NOISE INJECTION\n",
    "# Train\n",
    "save_imgs(X_train_transformed_noise, y_train_transformed_noise, genres, OUT_DIR+cropped_aug_inj, SPLITS[0])\n",
    "# Validation\n",
    "save_imgs(X_val_res, y_val_res, genres, OUT_DIR+cropped_aug_inj, SPLITS[1])\n",
    "# Test\n",
    "save_imgs(X_test_res, y_test_res, genres, OUT_DIR+cropped_aug_inj, SPLITS[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
